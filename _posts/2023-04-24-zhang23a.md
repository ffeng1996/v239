---
title: Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation
abstract: Recent advances in image tokenizers, such as VQ-VAE, have enabled text-to-image
  generation using auto-regressive methods, similar to language modeling. However,
  these methods have yet to leverage pre-trained language models, despite their adaptability
  to various downstream tasks. In this work, we explore this gap by adapting a pre-trained
  language model for auto-regressive text-to-image generation, and find that pre-trained
  language models offer limited help. We provide a two-fold explanation by analyzing
  tokens from each modality. First, we demonstrate that image tokens possess significantly
  different semantics compared to text tokens, rendering pre-trained language models
  no more effective in modeling them than randomly initialized ones. Second, the text
  tokens in the image-text datasets are too simple compared to normal language model
  pre-training data, which causes the catastrophic degradation of language modelsâ€™
  capability.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang23a
month: 0
tex_title: Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation
firstpage: 127
lastpage: 133
page: 127-133
order: 127
cycles: false
bibtex_author: Zhang, Yuhui and McKinzie, Brandon and Gan, Zhe and Shankar, Vaishaal
  and Toshev, Alexander
author:
- given: Yuhui
  family: Zhang
- given: Brandon
  family: McKinzie
- given: Zhe
  family: Gan
- given: Vaishaal
  family: Shankar
- given: Alexander
  family: Toshev
date: 2023-04-24
address:
container-title: 'Proceedings on "I Can''t Believe It''s Not Better: Failure  Modes
  in the Age of Foundation Models" at NeurIPS 2022 Workshops'
volume: '239'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 24
pdf: https://proceedings.mlr.press/v239/zhang23a/zhang23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
