---
title: Are large language models good annotators?
abstract: Numerous Natural Language Processing (NLP) tasks require precisely labeled
  data to ensure effective model training and achieve optimal performance. However,
  data annotation is marked by substantial costs and time requirements, especially
  when requiring specialized domain expertise or annotating a large number of samples.
  In this study, we investigate the feasibility of employing large language models
  (LLMs) as replacements for human annotators. We assess the zero-shot performance
  of various LLMs of different sizes to determine their viability as substitutes.
  Furthermore, recognizing that human annotators have access to diverse modalities,
  we introduce an image-based modality using the BLIP-2 architecture to evaluate LLM
  annotation performance. Among the tested LLMs, Vicuna-13b demonstrates competitive
  performance across diverse tasks. To assess the potential for LLMs to replace human
  annotators, we train a supervised model using labels generated by LLMs and compare
  its performance with models trained using human-generated labels. However, our findings
  reveal that models trained with human labels consistently outperform those trained
  with LLM-generated labels. We also highlights the challenges faced by LLMs in multilingual
  settings, where their performance significantly diminishes for tasks in languages
  other than English.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mohta23a
month: 0
tex_title: Are large language models good annotators?
firstpage: 38
lastpage: 48
page: 38-48
order: 38
cycles: false
bibtex_author: Mohta, Jay and Ak, Kenan and Xu, Yan and Shen, Mingwei
author:
- given: Jay
  family: Mohta
- given: Kenan
  family: Ak
- given: Yan
  family: Xu
- given: Mingwei
  family: Shen
date: 2023-04-24
address:
container-title: 'Proceedings on "I Can''t Believe It''s Not Better: Failure  Modes
  in the Age of Foundation Models" at NeurIPS 2022 Workshops'
volume: '239'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 24
pdf: https://proceedings.mlr.press/v239/mohta23a/mohta23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
