---
title: The Role of Linguistic Priors in Measuring Compositional Generalization of
  Vision-Language Models
abstract: 'Compositionality is a common property in many modalities including text
  and images, but the compositional generalization of multi-modal models is not well-understood.
  In this paper, we identify two sources of visual-linguistic compositionality: linguistic
  priors and the interplay between images and texts. We show that current attempts
  to improve compositional generalization rely on linguistic priors rather than on
  information in the image, as the strength of the language model in detecting sentences
  that are syntactically and semantically likely overwhelms the vision part of the
  model. We find in particular that a benchmark for compositionality mostly favors
  pure language models. Finally, we propose a new benchmark for compositionality without
  such linguistic priors'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu23a
month: 0
tex_title: The Role of Linguistic Priors in Measuring Compositional Generalization
  of Vision-Language Models
firstpage: 118
lastpage: 126
page: 118-126
order: 118
cycles: false
bibtex_author: Wu, Chenwei and Li, Li Erran and Ermon, Stefano and Haffner, Patrick
  and Ge, Rong and Zhang, Zaiwei
author:
- given: Chenwei
  family: Wu
- given: Li Erran
  family: Li
- given: Stefano
  family: Ermon
- given: Patrick
  family: Haffner
- given: Rong
  family: Ge
- given: Zaiwei
  family: Zhang
date: 2023-04-24
address:
container-title: 'Proceedings on "I Can''t Believe It''s Not Better: Failure  Modes
  in the Age of Foundation Models" at NeurIPS 2023 Workshops'
volume: '239'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 24
pdf: https://proceedings.mlr.press/v239/wu23a/wu23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
