---
title: Self-Evaluation Improves Selective Generation in Large Language Models
abstract: Safe deployment of large language models (LLMs) may benefit from a reliable
  method for assessing their generated content to determine when to abstain or to
  selectively generate. While likelihood-based metrics such as perplexity are widely
  employed, recent research has demonstrated the limitations of using sequence-level
  probability estimates given by LLMs as reliable indicators of generation quality.
  Conversely, LLMs have demonstrated strong calibration at the token level, particularly
  when it comes to choosing correct answers in multiple-choice questions or evaluating
  true/false statements. In this work, we reformulate open-ended generation tasks
  into token-level prediction tasks, and leverage LLMs’ superior calibration at the
  token level. We instruct an LLM to self-evaluate its answers, employing either a
  multi-way comparison or a point-wise evaluation approach, with the option to include
  an “None of the above” option to express the model’s uncertainty explicitly. We
  benchmark a range of scoring methods based on self-evaluation and evaluate their
  performance in selective generation using TruthfulQA and TL;DR. Through extensive
  experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores
  not only improve accuracy, but also correlate better with the overall quality of
  generated content.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ren23a
month: 0
tex_title: Self-Evaluation Improves Selective Generation in Large Language Models
firstpage: 49
lastpage: 64
page: 49-64
order: 49
cycles: false
bibtex_author: Ren, Jie and Zhao, Yao and Vu, Tu and Liu, Peter J. and Lakshminarayanan,
  Balaji
author:
- given: Jie
  family: Ren
- given: Yao
  family: Zhao
- given: Tu
  family: Vu
- given: Peter J.
  family: Liu
- given: Balaji
  family: Lakshminarayanan
date: 2023-04-24
address:
container-title: 'Proceedings on "I Can''t Believe It''s Not Better: Failure  Modes
  in the Age of Foundation Models" at NeurIPS 2023 Workshops'
volume: '239'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 24
pdf: https://proceedings.mlr.press/v239/ren23a/ren23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
