---
title: How (not) to ensemble LVLMs for VQA
abstract: 'This paper studies ensembling in the era of Large Vision-Language Models
  (LVLMs). Ensembling is a classical method to combine different models to get increased
  performance. In the recent work on Encyclopedic-VQA the authors examine a wide variety
  of models to solve their task: from vanilla LVLMs, to mod- els including the caption
  as extra context, to models augmented with Lens-based retrieval of Wikipedia pages.
  Intuitively these models are highly complementary, which should make them ideal
  for ensembling. Indeed, an oracle experiment (Fig. 1) shows potential gains from
  48.8% accuracy (the best single model) all the way up to 67% (best possible ensemble).
  So it is a trivial exercise to create an ensemble with substantial real gains. Or
  is it?'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: alazraki23a
month: 0
tex_title: How (not) to ensemble LVLMs for VQA
firstpage: 1
lastpage: 20
page: 1-20
order: 1
cycles: false
bibtex_author: Alazraki, Lisa and Castrejon, Lluis and Dehghani, Mostafa and Huot,
  Fantine and Uijlings, Jasper and Mensink, Thomas
author:
- given: Lisa
  family: Alazraki
- given: Lluis
  family: Castrejon
- given: Mostafa
  family: Dehghani
- given: Fantine
  family: Huot
- given: Jasper
  family: Uijlings
- given: Thomas
  family: Mensink
date: 2023-04-24
address:
container-title: 'Proceedings on "I Can''t Believe It''s Not Better: Failure  Modes
  in the Age of Foundation Models" at NeurIPS 2022 Workshops'
volume: '239'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 24
pdf: https://proceedings.mlr.press/v239/alazraki23a/alazraki23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
